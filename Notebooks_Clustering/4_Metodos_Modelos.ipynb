{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Métodos Basados en Modelos y Otros Algoritmos"],"metadata":{"id":"_EbBuI1lG3X3"}},{"cell_type":"markdown","metadata":{"id":"U_rMgo2OGutO"},"source":["\n","## GMM, Clustering Espectral, Mean-Shift, Affinity Propagation y BIRCH\n","\n","### Objetivos del Notebook\n","\n","1. Implementar Gaussian Mixture Models y comprender el algoritmo EM\n","2. Aplicar criterios de selección de modelos (BIC, AIC) para determinar el número de componentes\n","3. Dominar el clustering espectral para estructuras no convexas complejas\n","4. Explorar algoritmos complementarios: Mean-Shift, Affinity Propagation y BIRCH\n","5. Comparar el rendimiento de diferentes métodos en escenarios realistas\n","\n","---"]},{"cell_type":"markdown","metadata":{"id":"Z34FJKFUGutU"},"source":["## 1. Configuración del Entorno\n","\n","Importamos las bibliotecas necesarias para el desarrollo del módulo."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cMMRA1BHGutV"},"outputs":[],"source":["# Bibliotecas fundamentales\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from matplotlib.patches import Ellipse\n","import matplotlib.transforms as transforms\n","from mpl_toolkits.mplot3d import Axes3D\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","# Scikit-learn: clustering\n","from sklearn.mixture import GaussianMixture\n","from sklearn.cluster import (\n","    SpectralClustering, MeanShift, AffinityPropagation, Birch, KMeans\n",")\n","from sklearn.cluster import estimate_bandwidth\n","\n","# Scikit-learn: datasets y métricas\n","from sklearn.datasets import (\n","    make_blobs, make_moons, make_circles, load_iris, load_wine\n",")\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.decomposition import PCA\n","from sklearn.metrics import (\n","    silhouette_score, adjusted_rand_score, normalized_mutual_info_score\n",")\n","\n","# Configuración de visualización\n","plt.style.use('seaborn-v0_8-whitegrid')\n","plt.rcParams['figure.figsize'] = (12, 6)\n","plt.rcParams['font.size'] = 11\n","plt.rcParams['axes.titlesize'] = 13\n","plt.rcParams['axes.labelsize'] = 11\n","\n","# Reproducibilidad\n","RANDOM_STATE = 42\n","np.random.seed(RANDOM_STATE)\n","\n","print(\"Entorno configurado correctamente.\")"]},{"cell_type":"markdown","metadata":{"id":"mqOx8UTnGutX"},"source":["## 2. Gaussian Mixture Models (GMM)\n","\n","### 2.1 Concepto y Modelo Probabilístico\n","\n","Un GMM modela los datos como una mezcla de K distribuciones gaussianas:\n","\n","$$p(\\mathbf{x}) = \\sum_{k=1}^{K} \\pi_k \\cdot \\mathcal{N}(\\mathbf{x} | \\boldsymbol{\\mu}_k, \\boldsymbol{\\Sigma}_k)$$\n","\n","donde $\\pi_k$ son los pesos de mezcla (probabilidades a priori) y cada componente tiene su propia media $\\boldsymbol{\\mu}_k$ y covarianza $\\boldsymbol{\\Sigma}_k$."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rZvUrPBPGutY"},"outputs":[],"source":["# Generar datos con clusters elípticos de diferente orientación\n","np.random.seed(RANDOM_STATE)\n","\n","# Cluster 1: elipse orientada a 45 grados\n","n1 = 300\n","theta1 = np.pi / 4\n","cov1 = np.array([[2, 0], [0, 0.3]])\n","rotation1 = np.array([[np.cos(theta1), -np.sin(theta1)],\n","                      [np.sin(theta1), np.cos(theta1)]])\n","cov1_rotated = rotation1 @ cov1 @ rotation1.T\n","cluster1 = np.random.multivariate_normal([0, 0], cov1_rotated, n1)\n","\n","# Cluster 2: elipse orientada a -30 grados\n","n2 = 250\n","theta2 = -np.pi / 6\n","cov2 = np.array([[1.5, 0], [0, 0.4]])\n","rotation2 = np.array([[np.cos(theta2), -np.sin(theta2)],\n","                      [np.sin(theta2), np.cos(theta2)]])\n","cov2_rotated = rotation2 @ cov2 @ rotation2.T\n","cluster2 = np.random.multivariate_normal([4, 3], cov2_rotated, n2)\n","\n","# Cluster 3: circular\n","n3 = 100\n","cluster3 = np.random.multivariate_normal([2, -2], [[0.5, 0], [0, 0.5]], n3)\n","\n","# Combinar datos\n","X_ellipse = np.vstack([cluster1, cluster2, cluster3])\n","y_ellipse = np.array([0]*n1 + [1]*n2 + [2]*n3)\n","\n","print(f\"Dataset generado: {len(X_ellipse)} puntos en 3 clusters elípticos\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FGacc1YlGutZ"},"outputs":[],"source":["# Comparar K-Means vs GMM en datos elípticos\n","fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n","\n","# Ground truth\n","ax = axes[0]\n","ax.scatter(X_ellipse[:, 0], X_ellipse[:, 1], c=y_ellipse, cmap='viridis',\n","           edgecolors='w', s=40, alpha=0.7)\n","ax.set_title('Ground Truth')\n","ax.set_xlabel('Característica 1')\n","ax.set_ylabel('Característica 2')\n","\n","# K-Means\n","ax = axes[1]\n","kmeans = KMeans(n_clusters=3, random_state=RANDOM_STATE, n_init=10)\n","labels_km = kmeans.fit_predict(X_ellipse)\n","ari_km = adjusted_rand_score(y_ellipse, labels_km)\n","ax.scatter(X_ellipse[:, 0], X_ellipse[:, 1], c=labels_km, cmap='viridis',\n","           edgecolors='w', s=40, alpha=0.7)\n","ax.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1],\n","           c='red', marker='X', s=200, edgecolors='w', linewidths=2)\n","ax.set_title(f'K-Means (ARI: {ari_km:.3f})')\n","ax.set_xlabel('Característica 1')\n","ax.set_ylabel('Característica 2')\n","\n","# GMM\n","ax = axes[2]\n","gmm = GaussianMixture(n_components=3, covariance_type='full', random_state=RANDOM_STATE)\n","labels_gmm = gmm.fit_predict(X_ellipse)\n","ari_gmm = adjusted_rand_score(y_ellipse, labels_gmm)\n","ax.scatter(X_ellipse[:, 0], X_ellipse[:, 1], c=labels_gmm, cmap='viridis',\n","           edgecolors='w', s=40, alpha=0.7)\n","ax.scatter(gmm.means_[:, 0], gmm.means_[:, 1],\n","           c='red', marker='X', s=200, edgecolors='w', linewidths=2)\n","ax.set_title(f'GMM (ARI: {ari_gmm:.3f})')\n","ax.set_xlabel('Característica 1')\n","ax.set_ylabel('Característica 2')\n","\n","plt.suptitle('Comparación K-Means vs GMM en clusters elípticos', fontsize=14, fontweight='bold', y=1.02)\n","plt.tight_layout()\n","plt.show()\n","\n","print(f\"\\nK-Means asume clusters esféricos y falla con elipses orientadas.\")\n","print(f\"GMM captura la forma elíptica de cada cluster.\")"]},{"cell_type":"markdown","metadata":{"id":"18SXqZrdGutZ"},"source":["### 2.2 Tipos de Covarianza\n","\n","GMM permite diferentes restricciones en las matrices de covarianza, afectando la flexibilidad y el número de parámetros del modelo."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9Sn-rUjdGuta"},"outputs":[],"source":["def plot_gmm_ellipses(gmm, ax, colors):\n","    \"\"\"\n","    Dibuja las elipses de covarianza de un GMM.\n","    \"\"\"\n","    for i, (mean, covar, color) in enumerate(zip(gmm.means_, gmm.covariances_, colors)):\n","        if gmm.covariance_type == 'full':\n","            cov = covar\n","        elif gmm.covariance_type == 'tied':\n","            cov = gmm.covariances_\n","        elif gmm.covariance_type == 'diag':\n","            cov = np.diag(covar)\n","        elif gmm.covariance_type == 'spherical':\n","            cov = np.eye(2) * covar\n","\n","        # Calcular eigenvalores y eigenvectores\n","        eigenvalues, eigenvectors = np.linalg.eigh(cov)\n","        order = eigenvalues.argsort()[::-1]\n","        eigenvalues = eigenvalues[order]\n","        eigenvectors = eigenvectors[:, order]\n","\n","        # Ángulo de rotación\n","        angle = np.degrees(np.arctan2(eigenvectors[1, 0], eigenvectors[0, 0]))\n","\n","        # Dibujar elipses para 1, 2 y 3 desviaciones estándar\n","        for n_std in [1, 2]:\n","            width = 2 * n_std * np.sqrt(eigenvalues[0])\n","            height = 2 * n_std * np.sqrt(eigenvalues[1])\n","            ellipse = Ellipse(mean, width, height, angle=angle,\n","                            fill=False, edgecolor=color, linewidth=2,\n","                            alpha=0.8 if n_std == 1 else 0.4)\n","            ax.add_patch(ellipse)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FFklbWRwGutb"},"outputs":[],"source":["# Comparar tipos de covarianza\n","cov_types = ['full', 'tied', 'diag', 'spherical']\n","colors = ['#e74c3c', '#3498db', '#2ecc71']\n","\n","fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n","\n","for ax, cov_type in zip(axes.flatten(), cov_types):\n","    gmm = GaussianMixture(n_components=3, covariance_type=cov_type,\n","                          random_state=RANDOM_STATE, n_init=5)\n","    labels = gmm.fit_predict(X_ellipse)\n","    ari = adjusted_rand_score(y_ellipse, labels)\n","    bic = gmm.bic(X_ellipse)\n","\n","    ax.scatter(X_ellipse[:, 0], X_ellipse[:, 1], c=labels, cmap='viridis',\n","               edgecolors='w', s=30, alpha=0.6)\n","    plot_gmm_ellipses(gmm, ax, colors)\n","    ax.scatter(gmm.means_[:, 0], gmm.means_[:, 1], c='red', marker='X',\n","               s=150, edgecolors='w', linewidths=2, zorder=5)\n","\n","    ax.set_title(f'{cov_type.capitalize()}\\nARI: {ari:.3f}, BIC: {bic:.0f}')\n","    ax.set_xlabel('Característica 1')\n","    ax.set_ylabel('Característica 2')\n","    ax.set_xlim(-4, 8)\n","    ax.set_ylim(-5, 7)\n","\n","plt.suptitle('Tipos de Covarianza en GMM', fontsize=14, fontweight='bold', y=1.02)\n","plt.tight_layout()\n","plt.show()\n","\n","print(\"Interpretación de las elipses:\")\n","print(\"- Full: cada componente tiene su propia matriz de covarianza completa\")\n","print(\"- Tied: todas las componentes comparten la misma covarianza\")\n","print(\"- Diagonal: solo varianzas por eje (elipses alineadas con ejes)\")\n","print(\"- Spherical: varianza única por componente (círculos)\")"]},{"cell_type":"markdown","metadata":{"id":"1GmK6_75Gutc"},"source":["### 2.3 Selección del Número de Componentes con BIC y AIC"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TukvFPCNGutc"},"outputs":[],"source":["# Generar dataset más complejo para selección de modelo\n","np.random.seed(RANDOM_STATE)\n","\n","# 5 clusters con diferentes características\n","X_complex, y_complex = make_blobs(n_samples=600, centers=5,\n","                                   cluster_std=[0.8, 1.2, 0.6, 1.0, 0.9],\n","                                   random_state=RANDOM_STATE)\n","\n","# Calcular BIC y AIC para diferentes números de componentes\n","n_components_range = range(1, 12)\n","bics = []\n","aics = []\n","\n","for n in n_components_range:\n","    gmm = GaussianMixture(n_components=n, covariance_type='full',\n","                          random_state=RANDOM_STATE, n_init=5)\n","    gmm.fit(X_complex)\n","    bics.append(gmm.bic(X_complex))\n","    aics.append(gmm.aic(X_complex))\n","\n","# Visualización\n","fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n","\n","# BIC y AIC\n","ax = axes[0]\n","ax.plot(n_components_range, bics, 'b-o', label='BIC', linewidth=2, markersize=8)\n","ax.plot(n_components_range, aics, 'r--s', label='AIC', linewidth=2, markersize=8)\n","ax.axvline(x=np.argmin(bics)+1, color='blue', linestyle=':', alpha=0.7, label=f'BIC óptimo: {np.argmin(bics)+1}')\n","ax.axvline(x=np.argmin(aics)+1, color='red', linestyle=':', alpha=0.7, label=f'AIC óptimo: {np.argmin(aics)+1}')\n","ax.set_xlabel('Número de componentes')\n","ax.set_ylabel('Valor del criterio')\n","ax.set_title('Selección de K con BIC y AIC')\n","ax.legend()\n","ax.grid(True, alpha=0.3)\n","\n","# Ground truth\n","ax = axes[1]\n","ax.scatter(X_complex[:, 0], X_complex[:, 1], c=y_complex, cmap='viridis',\n","           edgecolors='w', s=40)\n","ax.set_title(f'Ground Truth (K=5)')\n","ax.set_xlabel('Característica 1')\n","ax.set_ylabel('Característica 2')\n","\n","# GMM con K óptimo según BIC\n","ax = axes[2]\n","k_optimo = np.argmin(bics) + 1\n","k_optimo = 4\n","gmm_optimo = GaussianMixture(n_components=k_optimo, covariance_type='full',\n","                              random_state=RANDOM_STATE, n_init=5)\n","labels_optimo = gmm_optimo.fit_predict(X_complex)\n","ari_optimo = adjusted_rand_score(y_complex, labels_optimo)\n","ax.scatter(X_complex[:, 0], X_complex[:, 1], c=labels_optimo, cmap='viridis',\n","           edgecolors='w', s=40)\n","ax.set_title(f'GMM con K={k_optimo} (BIC óptimo)\\nARI: {ari_optimo:.3f}')\n","ax.set_xlabel('Característica 1')\n","ax.set_ylabel('Característica 2')\n","\n","plt.tight_layout()\n","plt.show()\n","\n","print(f\"\\nK óptimo según BIC: {np.argmin(bics)+1}\")\n","print(f\"K óptimo según AIC: {np.argmin(aics)+1}\")\n","print(f\"K real: 5\")"]},{"cell_type":"markdown","metadata":{"id":"8AVyw_1zGutd"},"source":["### 2.4 Soft Clustering: Probabilidades de Pertenencia"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"69sEuR_1Gutd"},"outputs":[],"source":["# Entrenar GMM y obtener probabilidades\n","gmm_soft = GaussianMixture(n_components=3, covariance_type='full',\n","                           random_state=RANDOM_STATE)\n","gmm_soft.fit(X_ellipse)\n","\n","# Probabilidades de pertenencia (soft clustering)\n","probs = gmm_soft.predict_proba(X_ellipse)\n","\n","# Etiquetas (hard clustering)\n","labels_hard = gmm_soft.predict(X_ellipse)\n","\n","# Calcular incertidumbre (entropía normalizada)\n","def entropy(p):\n","    \"\"\"Calcula la entropía de una distribución de probabilidad.\"\"\"\n","    p = np.clip(p, 1e-10, 1)  # Evitar log(0)\n","    return -np.sum(p * np.log(p), axis=1)\n","\n","incertidumbre = entropy(probs) / np.log(3)  # Normalizado [0, 1]\n","\n","# Visualización\n","fig, axes = plt.subplots(2, 3, figsize=(14, 12))\n","\n","# Hard clustering\n","ax = axes[0, 0]\n","ax.scatter(X_ellipse[:, 0], X_ellipse[:, 1], c=labels_hard, cmap='viridis',\n","           edgecolors='w', s=40)\n","ax.set_title('Hard Clustering (etiqueta más probable)')\n","ax.set_xlabel('Característica 1')\n","ax.set_ylabel('Característica 2')\n","\n","# Incertidumbre\n","ax = axes[0, 1]\n","scatter = ax.scatter(X_ellipse[:, 0], X_ellipse[:, 1], c=incertidumbre,\n","                     cmap='Reds', edgecolors='w', s=40)\n","plt.colorbar(scatter, ax=ax, label='Incertidumbre (entropía normalizada)')\n","ax.set_title('Incertidumbre en la asignación')\n","ax.set_xlabel('Característica 1')\n","ax.set_ylabel('Característica 2')\n","\n","# Probabilidad para cada componente\n","for i in range(3):\n","    if i < 2:\n","        ax = axes[1, i]\n","        scatter = ax.scatter(X_ellipse[:, 0], X_ellipse[:, 1], c=probs[:, i],\n","                             cmap='Blues', edgecolors='w', s=40, vmin=0, vmax=1)\n","        plt.colorbar(scatter, ax=ax, label=f'P(componente {i})')\n","        ax.set_title(f'Probabilidad de pertenencia al componente {i}')\n","        ax.set_xlabel('Característica 1')\n","        ax.set_ylabel('Característica 2')\n","\n","plt.suptitle('Soft Clustering con GMM', fontsize=14, fontweight='bold', y=1.02)\n","plt.tight_layout()\n","plt.show()\n","\n","# Estadísticas de incertidumbre\n","print(f\"\\nEstadísticas de incertidumbre:\")\n","print(f\"  Media: {incertidumbre.mean():.3f}\")\n","print(f\"  Máxima: {incertidumbre.max():.3f}\")\n","print(f\"  Puntos con alta incertidumbre (>0.5): {(incertidumbre > 0.5).sum()}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"H7QZ1Uv6Gutd"},"outputs":[],"source":["# Ejemplo de puntos con diferentes niveles de certeza\n","print(\"Ejemplos de probabilidades de pertenencia:\\n\")\n","\n","# Punto con alta certeza\n","idx_alta_certeza = np.argmin(incertidumbre)\n","print(f\"Punto con ALTA certeza (índice {idx_alta_certeza}):\")\n","print(f\"  Probabilidades: {probs[idx_alta_certeza]}\")\n","print(f\"  Incertidumbre: {incertidumbre[idx_alta_certeza]:.4f}\")\n","\n","# Punto con baja certeza\n","idx_baja_certeza = np.argmax(incertidumbre)\n","print(f\"\\nPunto con BAJA certeza (índice {idx_baja_certeza}):\")\n","print(f\"  Probabilidades: {probs[idx_baja_certeza]}\")\n","print(f\"  Incertidumbre: {incertidumbre[idx_baja_certeza]:.4f}\")"]},{"cell_type":"markdown","metadata":{"id":"wh7Z7SlxGute"},"source":["## 3. Clustering Espectral\n","\n","### 3.1 Fundamentos: Datos No Convexos Complejos"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TjVTDiiJGute"},"outputs":[],"source":["# Crear dataset con estructuras no convexas complejas\n","np.random.seed(RANDOM_STATE)\n","\n","# Anillos concéntricos con ruido\n","n_samples = 400\n","t = np.linspace(0, 2*np.pi, n_samples // 2)\n","\n","# Anillo exterior\n","r1 = 2 + 0.1 * np.random.randn(n_samples // 2)\n","x1 = r1 * np.cos(t) + 0.1 * np.random.randn(n_samples // 2)\n","y1 = r1 * np.sin(t) + 0.1 * np.random.randn(n_samples // 2)\n","\n","# Anillo interior\n","r2 = 1 + 0.1 * np.random.randn(n_samples // 2)\n","x2 = r2 * np.cos(t) + 0.1 * np.random.randn(n_samples // 2)\n","y2 = r2 * np.sin(t) + 0.1 * np.random.randn(n_samples // 2)\n","\n","X_rings = np.vstack([np.column_stack([x1, y1]), np.column_stack([x2, y2])])\n","y_rings = np.array([0] * (n_samples // 2) + [1] * (n_samples // 2))\n","\n","# Espirales entrelazadas\n","n_spiral = 300\n","t_spiral = np.linspace(0, 3*np.pi, n_spiral)\n","noise = 0.3\n","\n","# Espiral 1\n","x1_spiral = t_spiral * np.cos(t_spiral) + noise * np.random.randn(n_spiral)\n","y1_spiral = t_spiral * np.sin(t_spiral) + noise * np.random.randn(n_spiral)\n","\n","# Espiral 2 (rotada 180 grados)\n","x2_spiral = t_spiral * np.cos(t_spiral + np.pi) + noise * np.random.randn(n_spiral)\n","y2_spiral = t_spiral * np.sin(t_spiral + np.pi) + noise * np.random.randn(n_spiral)\n","\n","X_spirals = np.vstack([np.column_stack([x1_spiral, y1_spiral]),\n","                       np.column_stack([x2_spiral, y2_spiral])])\n","y_spirals = np.array([0] * n_spiral + [1] * n_spiral)\n","\n","# Visualización\n","fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n","\n","ax = axes[0]\n","ax.scatter(X_rings[:, 0], X_rings[:, 1], c=y_rings, cmap='viridis',\n","           edgecolors='w', s=30)\n","ax.set_title('Anillos concéntricos')\n","ax.set_xlabel('X')\n","ax.set_ylabel('Y')\n","ax.set_aspect('equal')\n","\n","ax = axes[1]\n","ax.scatter(X_spirals[:, 0], X_spirals[:, 1], c=y_spirals, cmap='viridis',\n","           edgecolors='w', s=30)\n","ax.set_title('Espirales entrelazadas')\n","ax.set_xlabel('X')\n","ax.set_ylabel('Y')\n","ax.set_aspect('equal')\n","\n","plt.suptitle('Datasets con estructuras no convexas', fontsize=14, fontweight='bold')\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"25D8M16uGutf"},"outputs":[],"source":["# Comparar K-Means, GMM y Espectral en anillos\n","fig, axes = plt.subplots(2, 4, figsize=(18, 9))\n","\n","datasets = [\n","    (X_rings, y_rings, 'Anillos'),\n","    (X_spirals, y_spirals, 'Espirales')\n","]\n","\n","for row, (X, y_true, titulo) in enumerate(datasets):\n","    # Ground truth\n","    ax = axes[row, 0]\n","    ax.scatter(X[:, 0], X[:, 1], c=y_true, cmap='viridis', edgecolors='w', s=25)\n","    ax.set_title('Ground Truth')\n","    if row == 0:\n","        ax.set_ylabel(titulo, fontsize=12, fontweight='bold')\n","    ax.set_aspect('equal')\n","\n","    # K-Means\n","    ax = axes[row, 1]\n","    kmeans = KMeans(n_clusters=2, random_state=RANDOM_STATE, n_init=10)\n","    labels_km = kmeans.fit_predict(X)\n","    ari_km = adjusted_rand_score(y_true, labels_km)\n","    ax.scatter(X[:, 0], X[:, 1], c=labels_km, cmap='viridis', edgecolors='w', s=25)\n","    ax.set_title(f'K-Means\\nARI: {ari_km:.3f}')\n","    ax.set_aspect('equal')\n","\n","    # GMM\n","    ax = axes[row, 2]\n","    gmm = GaussianMixture(n_components=2, covariance_type='full', random_state=RANDOM_STATE)\n","    labels_gmm = gmm.fit_predict(X)\n","    ari_gmm = adjusted_rand_score(y_true, labels_gmm)\n","    ax.scatter(X[:, 0], X[:, 1], c=labels_gmm, cmap='viridis', edgecolors='w', s=25)\n","    ax.set_title(f'GMM\\nARI: {ari_gmm:.3f}')\n","    ax.set_aspect('equal')\n","\n","    # Espectral\n","    ax = axes[row, 3]\n","    spectral = SpectralClustering(n_clusters=2, affinity='nearest_neighbors',\n","                                   n_neighbors=10, random_state=RANDOM_STATE)\n","    labels_spec = spectral.fit_predict(X)\n","    ari_spec = adjusted_rand_score(y_true, labels_spec)\n","    ax.scatter(X[:, 0], X[:, 1], c=labels_spec, cmap='viridis', edgecolors='w', s=25)\n","    ax.set_title(f'Espectral\\nARI: {ari_spec:.3f}')\n","    ax.set_aspect('equal')\n","\n","plt.suptitle('Comparación de algoritmos en estructuras no convexas',\n","             fontsize=14, fontweight='bold', y=1.02)\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"kqVHevkXGutf"},"source":["### 3.2 Efecto del Parámetro de Afinidad"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Q-_hzd58Gutf"},"outputs":[],"source":["# Comparar diferentes configuraciones de clustering espectral\n","fig, axes = plt.subplots(2, 4, figsize=(18, 9))\n","\n","# Configuraciones a probar\n","configs = [\n","    {'affinity': 'rbf', 'gamma': 0.1, 'label': 'RBF (gamma=0.1)'},\n","    {'affinity': 'rbf', 'gamma': 1.0, 'label': 'RBF (gamma=1.0)'},\n","    {'affinity': 'rbf', 'gamma': 10.0, 'label': 'RBF (gamma=10)'},\n","    {'affinity': 'nearest_neighbors', 'n_neighbors': 10, 'label': 'k-NN (k=10)'}\n","]\n","\n","for row, (X, y_true, titulo) in enumerate(datasets):\n","    for col, config in enumerate(configs):\n","        ax = axes[row, col]\n","\n","        # Configurar clustering espectral\n","        if config['affinity'] == 'rbf':\n","            spectral = SpectralClustering(n_clusters=2, affinity='rbf',\n","                                          gamma=config['gamma'],\n","                                          random_state=RANDOM_STATE)\n","        else:\n","            spectral = SpectralClustering(n_clusters=2, affinity='nearest_neighbors',\n","                                          n_neighbors=config['n_neighbors'],\n","                                          random_state=RANDOM_STATE)\n","\n","        labels = spectral.fit_predict(X)\n","        ari = adjusted_rand_score(y_true, labels)\n","\n","        ax.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis', edgecolors='w', s=25)\n","        ax.set_title(f\"{config['label']}\\nARI: {ari:.3f}\")\n","        ax.set_aspect('equal')\n","\n","        if col == 0:\n","            ax.set_ylabel(titulo, fontsize=12, fontweight='bold')\n","\n","plt.suptitle('Efecto de los parámetros en Clustering Espectral',\n","             fontsize=14, fontweight='bold', y=1.02)\n","plt.tight_layout()\n","plt.show()\n","\n","print(\"Observaciones:\")\n","print(\"- gamma bajo (RBF): considera puntos más lejanos como similares\")\n","print(\"- gamma alto (RBF): solo puntos muy cercanos son similares\")\n","print(\"- k-NN: conectividad local basada en vecinos más cercanos\")"]},{"cell_type":"markdown","metadata":{"id":"RLV2Cjo8Gutg"},"source":["## 4. Otros Algoritmos de Clustering\n","\n","### 4.1 Mean-Shift"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8twG8DIsGutg"},"outputs":[],"source":["# Generar datos para Mean-Shift (detecta número de clusters automáticamente)\n","np.random.seed(RANDOM_STATE)\n","\n","# Clusters con diferentes densidades\n","X_ms, y_ms = make_blobs(n_samples=500, centers=4,\n","                        cluster_std=[0.5, 1.0, 0.7, 0.8],\n","                        random_state=RANDOM_STATE)\n","\n","# Estimar bandwidth automáticamente\n","bandwidth = estimate_bandwidth(X_ms, quantile=0.2)\n","print(f\"Bandwidth estimado: {bandwidth:.3f}\")\n","\n","# Aplicar Mean-Shift\n","ms = MeanShift(bandwidth=bandwidth, bin_seeding=True)\n","labels_ms = ms.fit_predict(X_ms)\n","n_clusters_ms = len(np.unique(labels_ms))\n","\n","# Visualización\n","fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n","\n","# Ground truth\n","ax = axes[0]\n","ax.scatter(X_ms[:, 0], X_ms[:, 1], c=y_ms, cmap='viridis', edgecolors='w', s=40)\n","ax.set_title('Ground Truth (K=4)')\n","ax.set_xlabel('Característica 1')\n","ax.set_ylabel('Característica 2')\n","\n","# Mean-Shift\n","ax = axes[1]\n","ax.scatter(X_ms[:, 0], X_ms[:, 1], c=labels_ms, cmap='viridis', edgecolors='w', s=40)\n","ax.scatter(ms.cluster_centers_[:, 0], ms.cluster_centers_[:, 1],\n","           c='red', marker='X', s=200, edgecolors='w', linewidths=2)\n","ari_ms = adjusted_rand_score(y_ms, labels_ms)\n","ax.set_title(f'Mean-Shift (K detectado: {n_clusters_ms})\\nARI: {ari_ms:.3f}')\n","ax.set_xlabel('Característica 1')\n","ax.set_ylabel('Característica 2')\n","\n","# Efecto del bandwidth\n","ax = axes[2]\n","bandwidths = [0.5, 1.0, 1.5, 2.0, 2.5, 3.0]\n","n_clusters_list = []\n","aris = []\n","\n","for bw in bandwidths:\n","    ms_test = MeanShift(bandwidth=bw, bin_seeding=True)\n","    labels_test = ms_test.fit_predict(X_ms)\n","    n_clusters_list.append(len(np.unique(labels_test)))\n","    aris.append(adjusted_rand_score(y_ms, labels_test))\n","\n","ax.plot(bandwidths, n_clusters_list, 'b-o', linewidth=2, markersize=8, label='Clusters')\n","ax.axhline(y=4, color='gray', linestyle='--', alpha=0.7, label='K real')\n","ax.axvline(x=bandwidth, color='red', linestyle=':', alpha=0.7, label=f'BW estimado: {bandwidth:.2f}')\n","ax.set_xlabel('Bandwidth')\n","ax.set_ylabel('Número de clusters')\n","ax.set_title('Efecto del bandwidth')\n","ax.legend()\n","ax.grid(True, alpha=0.3)\n","\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"WQbFEFY9Gutg"},"source":["### 4.2 Affinity Propagation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"c_u-cyhPGuth"},"outputs":[],"source":["# Dataset pequeño para Affinity Propagation (computacionalmente costoso)\n","X_ap, y_ap = make_blobs(n_samples=200, centers=5, cluster_std=0.8,\n","                        random_state=RANDOM_STATE)\n","\n","# Probar diferentes valores de preference\n","preferences = [None, -50, -100, -200]\n","\n","fig, axes = plt.subplots(1, 4, figsize=(18, 4))\n","\n","for ax, pref in zip(axes, preferences):\n","    ap = AffinityPropagation(damping=0.9, preference=pref, random_state=RANDOM_STATE)\n","    labels_ap = ap.fit_predict(X_ap)\n","    n_clusters_ap = len(np.unique(labels_ap))\n","    ari_ap = adjusted_rand_score(y_ap, labels_ap)\n","\n","    # Ejemplares (puntos representativos)\n","    ejemplares = ap.cluster_centers_indices_\n","\n","    ax.scatter(X_ap[:, 0], X_ap[:, 1], c=labels_ap, cmap='viridis',\n","               edgecolors='w', s=40)\n","    ax.scatter(X_ap[ejemplares, 0], X_ap[ejemplares, 1],\n","               c='red', marker='X', s=200, edgecolors='w', linewidths=2)\n","\n","    pref_str = 'Auto' if pref is None else str(pref)\n","    ax.set_title(f'Preference: {pref_str}\\nK: {n_clusters_ap}, ARI: {ari_ap:.3f}')\n","    ax.set_xlabel('Característica 1')\n","    ax.set_ylabel('Característica 2')\n","\n","plt.suptitle('Affinity Propagation: Efecto del parámetro preference',\n","             fontsize=14, fontweight='bold', y=1.05)\n","plt.tight_layout()\n","plt.show()\n","\n","print(\"Nota: Los ejemplares (X rojas) son puntos reales del dataset, no centroides calculados.\")\n","print(\"Preference más negativo = menos clusters.\")"]},{"cell_type":"markdown","metadata":{"id":"tBVdYQQSGuth"},"source":["### 4.3 BIRCH para Grandes Volúmenes de Datos"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jkzE3Ir6Guth"},"outputs":[],"source":["# Generar dataset grande\n","np.random.seed(RANDOM_STATE)\n","n_large = 10000\n","\n","X_large, y_large = make_blobs(n_samples=n_large, centers=6,\n","                               cluster_std=1.0, random_state=RANDOM_STATE)\n","\n","print(f\"Dataset generado: {n_large} puntos, 6 clusters\")\n","\n","# Comparar tiempos de ejecución\n","import time\n","\n","resultados = []\n","\n","# BIRCH\n","start = time.time()\n","birch = Birch(n_clusters=6, threshold=0.5, branching_factor=50)\n","labels_birch = birch.fit_predict(X_large)\n","tiempo_birch = time.time() - start\n","ari_birch = adjusted_rand_score(y_large, labels_birch)\n","resultados.append(('BIRCH', tiempo_birch, ari_birch))\n","\n","# K-Means\n","start = time.time()\n","kmeans_large = KMeans(n_clusters=6, random_state=RANDOM_STATE, n_init=10)\n","labels_kmeans_large = kmeans_large.fit_predict(X_large)\n","tiempo_kmeans = time.time() - start\n","ari_kmeans = adjusted_rand_score(y_large, labels_kmeans_large)\n","resultados.append(('K-Means', tiempo_kmeans, ari_kmeans))\n","\n","# GMM\n","start = time.time()\n","gmm_large = GaussianMixture(n_components=6, random_state=RANDOM_STATE, n_init=3)\n","labels_gmm_large = gmm_large.fit_predict(X_large)\n","tiempo_gmm = time.time() - start\n","ari_gmm = adjusted_rand_score(y_large, labels_gmm_large)\n","resultados.append(('GMM', tiempo_gmm, ari_gmm))\n","\n","# Mostrar resultados\n","print(\"\\nComparación de rendimiento (n=10,000):\")\n","print(\"=\" * 45)\n","print(f\"{'Algoritmo':<15} {'Tiempo (s)':<15} {'ARI':<10}\")\n","print(\"-\" * 45)\n","for nombre, tiempo, ari in resultados:\n","    print(f\"{nombre:<15} {tiempo:<15.4f} {ari:<10.4f}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7GGgzCPBGuth"},"outputs":[],"source":["# Visualización del clustering en dataset grande\n","fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n","\n","algoritmos = [\n","    ('BIRCH', labels_birch, ari_birch, tiempo_birch),\n","    ('K-Means', labels_kmeans_large, ari_kmeans, tiempo_kmeans),\n","    ('GMM', labels_gmm_large, ari_gmm, tiempo_gmm)\n","]\n","\n","for ax, (nombre, labels, ari, tiempo) in zip(axes, algoritmos):\n","    # Submuestrear para visualización\n","    idx = np.random.choice(len(X_large), 2000, replace=False)\n","    ax.scatter(X_large[idx, 0], X_large[idx, 1], c=labels[idx],\n","               cmap='viridis', s=10, alpha=0.6)\n","    ax.set_title(f'{nombre}\\nARI: {ari:.3f}, Tiempo: {tiempo:.3f}s')\n","    ax.set_xlabel('Característica 1')\n","    ax.set_ylabel('Característica 2')\n","\n","plt.suptitle(f'Comparación en dataset grande (n={n_large})',\n","             fontsize=14, fontweight='bold', y=1.02)\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LIEwWIUGGuti"},"outputs":[],"source":["# Escalabilidad de BIRCH vs otros algoritmos\n","tamanios = [500, 1000, 2000, 5000, 10000, 20000]\n","tiempos_birch = []\n","tiempos_kmeans = []\n","\n","for n in tamanios:\n","    X_test, _ = make_blobs(n_samples=n, centers=6, random_state=RANDOM_STATE)\n","\n","    # BIRCH\n","    start = time.time()\n","    Birch(n_clusters=6).fit_predict(X_test)\n","    tiempos_birch.append(time.time() - start)\n","\n","    # K-Means\n","    start = time.time()\n","    KMeans(n_clusters=6, n_init=3, random_state=RANDOM_STATE).fit_predict(X_test)\n","    tiempos_kmeans.append(time.time() - start)\n","\n","# Visualización de escalabilidad\n","plt.figure(figsize=(10, 6))\n","plt.plot(tamanios, tiempos_birch, 'b-o', linewidth=2, markersize=8, label='BIRCH')\n","plt.plot(tamanios, tiempos_kmeans, 'r-s', linewidth=2, markersize=8, label='K-Means')\n","plt.xlabel('Número de muestras')\n","plt.ylabel('Tiempo (segundos)')\n","plt.title('Escalabilidad: BIRCH vs K-Means')\n","plt.legend()\n","plt.grid(True, alpha=0.3)\n","plt.tight_layout()\n","plt.show()\n","\n","print(\"BIRCH escala linealmente O(n), ideal para datasets muy grandes.\")"]},{"cell_type":"markdown","metadata":{"id":"ucBCwLb-Guti"},"source":["## 5. Caso Práctico: Dataset Wine\n","\n","Aplicamos los diferentes algoritmos a un dataset real multidimensional."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"juon7EW9Guti"},"outputs":[],"source":["# Cargar dataset Wine\n","wine = load_wine()\n","X_wine = wine.data\n","y_wine = wine.target\n","\n","print(f\"Dataset Wine:\")\n","print(f\"  Muestras: {X_wine.shape[0]}\")\n","print(f\"  Características: {X_wine.shape[1]}\")\n","print(f\"  Clases: {len(np.unique(y_wine))} ({np.unique(y_wine)})\")\n","print(f\"\\nCaracterísticas: {wine.feature_names}\")\n","\n","# Estandarizar\n","scaler = StandardScaler()\n","X_wine_scaled = scaler.fit_transform(X_wine)\n","\n","# Reducir a 2D para visualización\n","pca = PCA(n_components=2)\n","X_wine_2d = pca.fit_transform(X_wine_scaled)\n","print(f\"\\nVarianza explicada por 2 componentes: {pca.explained_variance_ratio_.sum():.2%}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"T26Vt2QdGuti"},"outputs":[],"source":["# Selección de K para GMM usando BIC\n","n_components_range = range(1, 10)\n","bics_wine = []\n","\n","for n in n_components_range:\n","    gmm = GaussianMixture(n_components=n, covariance_type='full',\n","                          random_state=RANDOM_STATE, n_init=5)\n","    gmm.fit(X_wine_scaled)\n","    bics_wine.append(gmm.bic(X_wine_scaled))\n","\n","plt.figure(figsize=(10, 5))\n","plt.plot(n_components_range, bics_wine, 'b-o', linewidth=2, markersize=8)\n","plt.axvline(x=np.argmin(bics_wine)+1, color='red', linestyle='--',\n","            label=f'K óptimo: {np.argmin(bics_wine)+1}')\n","plt.axvline(x=3, color='green', linestyle=':', alpha=0.7, label='K real: 3')\n","plt.xlabel('Número de componentes')\n","plt.ylabel('BIC')\n","plt.title('Selección de K con BIC para dataset Wine')\n","plt.legend()\n","plt.grid(True, alpha=0.3)\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uVvKT3WkGutj"},"outputs":[],"source":["# Comparación completa de algoritmos en Wine\n","fig, axes = plt.subplots(2, 4, figsize=(18, 9))\n","k = 3\n","\n","# Definir algoritmos\n","algoritmos = [\n","    ('Ground Truth', None, y_wine),\n","    ('K-Means', KMeans(n_clusters=k, random_state=RANDOM_STATE, n_init=10), None),\n","    ('GMM', GaussianMixture(n_components=k, covariance_type='full', random_state=RANDOM_STATE), None),\n","    ('Espectral', SpectralClustering(n_clusters=k, affinity='rbf', gamma=0.1, random_state=RANDOM_STATE), None),\n","    ('Mean-Shift', MeanShift(bandwidth=estimate_bandwidth(X_wine_scaled, quantile=0.3)), None),\n","    ('Affinity Prop.', AffinityPropagation(damping=0.9, preference=-50, random_state=RANDOM_STATE), None),\n","    ('BIRCH', Birch(n_clusters=k, threshold=1.0), None),\n","]\n","\n","resultados_wine = []\n","\n","for idx, (nombre, modelo, labels_predef) in enumerate(algoritmos):\n","    row = idx // 4\n","    col = idx % 4\n","    ax = axes[row, col]\n","\n","    if labels_predef is not None:\n","        labels = labels_predef\n","        ari = 1.0\n","        nmi = 1.0\n","    else:\n","        labels = modelo.fit_predict(X_wine_scaled)\n","        ari = adjusted_rand_score(y_wine, labels)\n","        nmi = normalized_mutual_info_score(y_wine, labels)\n","        resultados_wine.append((nombre, ari, nmi, len(np.unique(labels))))\n","\n","    ax.scatter(X_wine_2d[:, 0], X_wine_2d[:, 1], c=labels, cmap='viridis',\n","               edgecolors='w', s=40, alpha=0.7)\n","\n","    if nombre == 'Ground Truth':\n","        ax.set_title(f'{nombre}')\n","    else:\n","        n_clusters = len(np.unique(labels))\n","        ax.set_title(f'{nombre}\\nK={n_clusters}, ARI={ari:.3f}')\n","\n","    ax.set_xlabel('PC1')\n","    ax.set_ylabel('PC2')\n","\n","# Ocultar el último subplot\n","axes[1, 3].axis('off')\n","\n","plt.suptitle('Comparación de algoritmos en dataset Wine',\n","             fontsize=14, fontweight='bold', y=1.02)\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fjJzJmcpGutj"},"outputs":[],"source":["# Tabla comparativa de resultados\n","print(\"\\nResultados comparativos en dataset Wine:\")\n","print(\"=\" * 55)\n","print(f\"{'Algoritmo':<18} {'ARI':<10} {'NMI':<10} {'K detectado':<12}\")\n","print(\"-\" * 55)\n","for nombre, ari, nmi, k in resultados_wine:\n","    print(f\"{nombre:<18} {ari:<10.4f} {nmi:<10.4f} {k:<12}\")\n","print(\"-\" * 55)\n","print(f\"{'(K real = 3)':<18}\")"]},{"cell_type":"markdown","metadata":{"id":"pkOnUEA3Gutj"},"source":["## 6. Caso Práctico Avanzado: Segmentación de Imágenes\n","\n","Aplicamos GMM para segmentar una imagen basándose en sus colores."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hynN9LOSGutk"},"outputs":[],"source":["# Generar imagen sintética con regiones de colores\n","np.random.seed(RANDOM_STATE)\n","\n","# Crear imagen 100x100 con 4 regiones de colores\n","img_size = 100\n","img = np.zeros((img_size, img_size, 3))\n","\n","# Región 1: Rojo (esquina superior izquierda)\n","img[:50, :50] = [0.8, 0.2, 0.1] + np.random.randn(50, 50, 3) * 0.05\n","\n","# Región 2: Verde (esquina superior derecha)\n","img[:50, 50:] = [0.2, 0.7, 0.2] + np.random.randn(50, 50, 3) * 0.05\n","\n","# Región 3: Azul (esquina inferior izquierda)\n","img[50:, :50] = [0.1, 0.3, 0.8] + np.random.randn(50, 50, 3) * 0.05\n","\n","# Región 4: Amarillo (esquina inferior derecha)\n","img[50:, 50:] = [0.9, 0.8, 0.2] + np.random.randn(50, 50, 3) * 0.05\n","\n","# Clipear valores\n","img = np.clip(img, 0, 1)\n","\n","# Añadir un círculo de otro color en el centro\n","center = (50, 50)\n","radius = 20\n","for i in range(img_size):\n","    for j in range(img_size):\n","        if (i - center[0])**2 + (j - center[1])**2 < radius**2:\n","            img[i, j] = [0.5, 0.5, 0.5] + np.random.randn(3) * 0.03\n","\n","img = np.clip(img, 0, 1)\n","\n","plt.figure(figsize=(6, 6))\n","plt.imshow(img)\n","plt.title('Imagen original')\n","plt.axis('off')\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"POm0sdLEGutk"},"outputs":[],"source":["# Preparar datos para clustering\n","# Cada pixel es un punto en espacio RGB 3D\n","X_img = img.reshape(-1, 3)\n","\n","print(f\"Datos de imagen: {X_img.shape[0]} píxeles, {X_img.shape[1]} canales (RGB)\")\n","\n","# Selección de K con BIC\n","n_components_range = range(2, 10)\n","bics_img = []\n","\n","for n in n_components_range:\n","    gmm = GaussianMixture(n_components=n, covariance_type='full',\n","                          random_state=RANDOM_STATE, n_init=3)\n","    gmm.fit(X_img)\n","    bics_img.append(gmm.bic(X_img))\n","\n","k_optimo_img = n_components_range[np.argmin(bics_img)]\n","\n","plt.figure(figsize=(10, 5))\n","plt.plot(list(n_components_range), bics_img, 'b-o', linewidth=2, markersize=8)\n","plt.axvline(x=k_optimo_img, color='red', linestyle='--',\n","            label=f'K óptimo: {k_optimo_img}')\n","plt.xlabel('Número de componentes')\n","plt.ylabel('BIC')\n","plt.title('Selección de K para segmentación de imagen')\n","plt.legend()\n","plt.grid(True, alpha=0.3)\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ms52CxkDGutk"},"outputs":[],"source":["# Segmentación con GMM\n","gmm_img = GaussianMixture(n_components=k_optimo_img, covariance_type='full',\n","                          random_state=RANDOM_STATE, n_init=5)\n","labels_img = gmm_img.fit_predict(X_img)\n","probs_img = gmm_img.predict_proba(X_img)\n","\n","# Reconstruir imagen segmentada\n","segmented = labels_img.reshape(img_size, img_size)\n","\n","# Imagen con colores promedio de cada segmento\n","img_reconstructed = np.zeros_like(img)\n","for k in range(k_optimo_img):\n","    mask = labels_img == k\n","    color_mean = X_img[mask].mean(axis=0)\n","    img_reconstructed.reshape(-1, 3)[mask] = color_mean\n","\n","# Visualización\n","fig, axes = plt.subplots(1, 4, figsize=(18, 4))\n","\n","# Original\n","ax = axes[0]\n","ax.imshow(img)\n","ax.set_title('Imagen original')\n","ax.axis('off')\n","\n","# Segmentación (etiquetas)\n","ax = axes[1]\n","ax.imshow(segmented, cmap='tab10')\n","ax.set_title(f'Segmentación GMM (K={k_optimo_img})')\n","ax.axis('off')\n","\n","# Imagen reconstruida\n","ax = axes[2]\n","ax.imshow(img_reconstructed)\n","ax.set_title('Imagen reconstruida\\n(colores promedio)')\n","ax.axis('off')\n","\n","# Incertidumbre\n","ax = axes[3]\n","incertidumbre_img = entropy(probs_img) / np.log(k_optimo_img)\n","ax.imshow(incertidumbre_img.reshape(img_size, img_size), cmap='Reds')\n","ax.set_title('Mapa de incertidumbre')\n","ax.axis('off')\n","\n","plt.suptitle('Segmentación de imagen con GMM', fontsize=14, fontweight='bold', y=1.02)\n","plt.tight_layout()\n","plt.show()\n","\n","print(f\"\\nColores medios de cada segmento (RGB):\")\n","for k in range(k_optimo_img):\n","    mask = labels_img == k\n","    color_mean = X_img[mask].mean(axis=0)\n","    print(f\"  Segmento {k}: [{color_mean[0]:.2f}, {color_mean[1]:.2f}, {color_mean[2]:.2f}]\")"]},{"cell_type":"markdown","metadata":{"id":"J3Vx_O6LGutk"},"source":["## 7. Resumen y Conclusiones\n","\n","### Conceptos Clave\n","\n","1. **Gaussian Mixture Models (GMM)**\n","   - Modelo probabilístico que generaliza K-Means\n","   - Permite clusters elípticos con diferentes orientaciones\n","   - Soft clustering: proporciona probabilidades de pertenencia\n","   - Selección de K mediante BIC/AIC\n","\n","2. **Clustering Espectral**\n","   - Transforma datos usando eigenvectores del laplaciano del grafo\n","   - Excelente para estructuras no convexas (anillos, espirales)\n","   - Parámetros críticos: tipo de afinidad (RBF, k-NN), gamma/n_neighbors\n","\n","3. **Mean-Shift**\n","   - Encuentra modos de la densidad de datos\n","   - No requiere especificar K\n","   - Parámetro crítico: bandwidth\n","\n","4. **Affinity Propagation**\n","   - Selecciona ejemplares (puntos reales) como representantes\n","   - No requiere K pero sensible a preference\n","\n","5. **BIRCH**\n","   - Optimizado para grandes volúmenes de datos\n","   - Complejidad lineal O(n)\n","   - Ideal cuando los datos no caben en memoria\n","\n","### Guía de Selección\n","\n","| Situación | Algoritmo Recomendado |\n","|-----------|-----------------------|\n","| Clusters elípticos | GMM (covariance_type='full') |\n","| Soft clustering | GMM |\n","| Estructuras no convexas | Espectral |\n","| K desconocido | Mean-Shift, Affinity Propagation |\n","| Dataset muy grande | BIRCH |\n","| Ejemplares reales necesarios | Affinity Propagation |"]},{"cell_type":"markdown","metadata":{"id":"W63i7xF6Gutl"},"source":["---\n","\n","## Referencias\n","\n","- Bishop, C. M. (2006). Pattern Recognition and Machine Learning. Springer. (Capítulo 9: Mixture Models and EM)\n","- von Luxburg, U. (2007). A tutorial on spectral clustering. Statistics and Computing, 17(4), 395-416.\n","- Comaniciu, D., & Meer, P. (2002). Mean shift: A robust approach toward feature space analysis. IEEE TPAMI.\n","- Frey, B. J., & Dueck, D. (2007). Clustering by passing messages between data points. Science, 315(5814), 972-976.\n","- Zhang, T., Ramakrishnan, R., & Livny, M. (1996). BIRCH: an efficient data clustering method for very large databases. ACM SIGMOD.\n","- Scikit-learn documentation: https://scikit-learn.org/stable/modules/clustering.html\n","\n","---\n"]},{"cell_type":"markdown","source":["# EOF (End Of File)"],"metadata":{"id":"sVOaDCuPMJ2g"}}]}