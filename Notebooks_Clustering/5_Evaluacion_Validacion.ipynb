{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true,"collapsed_sections":["2yTPv-YDW3fi","6korq2mQW3fm","_il-ueobW3fv","ZjKwgn_KW3f0","pJ-KJvvoW3f1","iSfdtmIBW3f4","ubZQ7wpQW3gC"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Evaluación y Validación de Clusters"],"metadata":{"id":"732ryKU_XAjo"}},{"cell_type":"markdown","metadata":{"id":"pPyJxpjwW3fb"},"source":["\n","## Métricas Internas, Métricas Externas y Estabilidad\n","\n","---\n","### Objetivos del Notebook\n","\n","1. Calcular e interpretar métricas internas: Silueta, Calinski-Harabasz y Davies-Bouldin\n","2. Visualizar el análisis de silueta por cluster para diagnóstico detallado\n","3. Aplicar métricas externas: ARI, NMI, V-measure y Fowlkes-Mallows\n","4. Evaluar la estabilidad de clusters mediante técnicas de bootstrap\n","5. Utilizar métricas para selección del número óptimo de clusters\n","\n","---"]},{"cell_type":"markdown","metadata":{"id":"2yTPv-YDW3fi"},"source":["## 1. Configuración del Entorno"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"B-mDojygW3fk"},"outputs":[],"source":["# Bibliotecas fundamentales\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import matplotlib.cm as cm\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","# Scikit-learn: clustering\n","from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN\n","from sklearn.mixture import GaussianMixture\n","\n","# Scikit-learn: métricas internas\n","from sklearn.metrics import (\n","    silhouette_score,\n","    silhouette_samples,\n","    calinski_harabasz_score,\n","    davies_bouldin_score\n",")\n","\n","# Scikit-learn: métricas externas\n","from sklearn.metrics import (\n","    adjusted_rand_score,\n","    normalized_mutual_info_score,\n","    homogeneity_score,\n","    completeness_score,\n","    v_measure_score,\n","    fowlkes_mallows_score\n",")\n","\n","# Scikit-learn: datasets y preprocesamiento\n","from sklearn.datasets import make_blobs, make_moons, load_iris, load_wine\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.decomposition import PCA\n","\n","# Configuración de visualización\n","plt.style.use('seaborn-v0_8-whitegrid')\n","plt.rcParams['figure.figsize'] = (12, 6)\n","plt.rcParams['font.size'] = 11\n","plt.rcParams['axes.titlesize'] = 13\n","plt.rcParams['axes.labelsize'] = 11\n","\n","# Reproducibilidad\n","RANDOM_STATE = 42\n","np.random.seed(RANDOM_STATE)\n","\n","print(\"Entorno configurado correctamente.\")"]},{"cell_type":"markdown","metadata":{"id":"6korq2mQW3fm"},"source":["## 2. Métricas Internas: Fundamentos\n","\n","Las métricas internas evalúan la calidad del clustering utilizando únicamente los datos y las asignaciones, sin requerir etiquetas verdaderas (ground truth)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1G4FJ9wUW3fn"},"outputs":[],"source":["# Generar datasets con diferentes estructuras de clustering\n","np.random.seed(RANDOM_STATE)\n","\n","# Dataset 1: Clusters bien separados\n","X_separados, y_separados = make_blobs(n_samples=300, centers=3,\n","                                       cluster_std=0.5, random_state=RANDOM_STATE)\n","\n","# Dataset 2: Clusters solapados\n","X_solapados, y_solapados = make_blobs(n_samples=300, centers=3,\n","                                       cluster_std=4.0, random_state=RANDOM_STATE)\n","\n","# Dataset 3: Clusters de diferentes tamaños y densidades\n","X1 = np.random.randn(150, 2) * 0.5 + [0, 0]\n","X2 = np.random.randn(50, 2) * 0.3 + [4, 4]\n","X3 = np.random.randn(100, 2) * 1.5 + [-3, 4]\n","X_desbalanceado = np.vstack([X1, X2, X3])\n","y_desbalanceado = np.array([0]*150 + [1]*50 + [2]*100)\n","\n","# Visualizar los datasets\n","fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n","\n","datasets = [\n","    (X_separados, y_separados, 'Clusters bien separados'),\n","    (X_solapados, y_solapados, 'Clusters solapados'),\n","    (X_desbalanceado, y_desbalanceado, 'Clusters desbalanceados')\n","]\n","\n","for ax, (X, y, titulo) in zip(axes, datasets):\n","    ax.scatter(X[:, 0], X[:, 1], c=y, cmap='viridis', edgecolors='w', s=40)\n","    ax.set_title(titulo)\n","    ax.set_xlabel('X1')\n","    ax.set_ylabel('X2')\n","\n","plt.suptitle('Datasets de ejemplo para evaluación', fontsize=14, fontweight='bold')\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"gNLEWgtLW3fo"},"source":["### 2.1 Coeficiente de Silueta (Silhouette Score)\n","\n","El coeficiente de silueta mide qué tan similar es un punto a su propio cluster comparado con otros clusters.\n","\n","$$s(i) = \\frac{b(i) - a(i)}{\\max(a(i), b(i))}$$\n","\n","donde:\n","- $a(i)$: distancia media a puntos del mismo cluster\n","- $b(i)$: distancia media al cluster más cercano"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"83HrVevcW3fp"},"outputs":[],"source":["# Calcular silueta para cada dataset con K-Means\n","print(\"Coeficiente de Silueta (Silhouette Score)\")\n","print(\"=\" * 50)\n","print(\"Rango: [-1, 1], Mayor = Mejor\")\n","print(\"=\" * 50)\n","\n","for X, y, titulo in datasets:\n","    kmeans = KMeans(n_clusters=3, random_state=RANDOM_STATE, n_init=10)\n","    labels = kmeans.fit_predict(X)\n","\n","    # Silueta global\n","    silueta_global = silhouette_score(X, labels)\n","\n","    # Silueta por punto\n","    siluetas_individuales = silhouette_samples(X, labels)\n","\n","    print(f\"\\n{titulo}:\")\n","    print(f\"  Silueta global: {silueta_global:.4f}\")\n","    print(f\"  Silueta media por cluster:\")\n","    for i in range(3):\n","        mask = labels == i\n","        print(f\"    Cluster {i}: {siluetas_individuales[mask].mean():.4f} (n={mask.sum()})\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"71qojHg2W3fq"},"outputs":[],"source":["def plot_silhouette_analysis(X, labels, titulo):\n","    \"\"\"\n","    Genera un gráfico de análisis de silueta detallado por cluster.\n","    \"\"\"\n","    n_clusters = len(np.unique(labels))\n","    silhouette_avg = silhouette_score(X, labels)\n","    sample_silhouette_values = silhouette_samples(X, labels)\n","\n","    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n","\n","    # Gráfico de silueta\n","    y_lower = 10\n","    colors = cm.viridis(np.linspace(0, 1, n_clusters))\n","\n","    for i in range(n_clusters):\n","        # Obtener siluetas del cluster i y ordenarlas\n","        ith_cluster_silhouette_values = sample_silhouette_values[labels == i]\n","        ith_cluster_silhouette_values.sort()\n","\n","        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n","        y_upper = y_lower + size_cluster_i\n","\n","        ax1.fill_betweenx(np.arange(y_lower, y_upper),\n","                          0, ith_cluster_silhouette_values,\n","                          facecolor=colors[i], edgecolor=colors[i], alpha=0.7)\n","\n","        # Etiqueta del cluster\n","        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i), fontsize=10)\n","        y_lower = y_upper + 10\n","\n","    # Línea vertical para la silueta promedio\n","    ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\",\n","                label=f'Silueta media: {silhouette_avg:.3f}')\n","    ax1.axvline(x=0, color=\"gray\", linestyle=\"-\", alpha=0.5)\n","\n","    ax1.set_xlabel('Coeficiente de silueta')\n","    ax1.set_ylabel('Cluster')\n","    ax1.set_title('Análisis de silueta por cluster')\n","    ax1.set_xlim([-0.3, 1])\n","    ax1.legend(loc='upper right')\n","\n","    # Gráfico de dispersión con colores\n","    ax2.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis',\n","                edgecolors='w', s=40)\n","    ax2.set_xlabel('X1')\n","    ax2.set_ylabel('X2')\n","    ax2.set_title('Asignación de clusters')\n","\n","    plt.suptitle(titulo, fontsize=14, fontweight='bold')\n","    plt.tight_layout()\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"E3PxAUwyW3fr"},"outputs":[],"source":["# Análisis de silueta visual para cada dataset\n","for X, y, titulo in datasets:\n","    kmeans = KMeans(n_clusters=3, random_state=RANDOM_STATE, n_init=10)\n","    labels = kmeans.fit_predict(X)\n","    plot_silhouette_analysis(X, labels, titulo)"]},{"cell_type":"markdown","metadata":{"id":"lHtUUcZIW3fs"},"source":["### 2.2 Índice de Calinski-Harabasz\n","\n","También conocido como Variance Ratio Criterion, mide la relación entre la dispersión inter-cluster y la dispersión intra-cluster.\n","\n","$$CH = \\frac{B / (K-1)}{W / (n-K)}$$\n","\n","donde B es la dispersión between-cluster y W la dispersión within-cluster."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JNKZj31eW3ft"},"outputs":[],"source":["# Calcular Calinski-Harabasz para cada dataset\n","print(\"Índice de Calinski-Harabasz\")\n","print(\"=\" * 50)\n","print(\"Rango: [0, +inf), Mayor = Mejor\")\n","print(\"=\" * 50)\n","\n","for X, y, titulo in datasets:\n","    kmeans = KMeans(n_clusters=3, random_state=RANDOM_STATE, n_init=10)\n","    labels = kmeans.fit_predict(X)\n","    ch_score = calinski_harabasz_score(X, labels)\n","    print(f\"\\n{titulo}: {ch_score:.2f}\")"]},{"cell_type":"markdown","metadata":{"id":"dxc_cERRW3ft"},"source":["### 2.3 Índice de Davies-Bouldin\n","\n","Mide la similitud promedio entre cada cluster y su cluster más parecido.\n","\n","$$DB = \\frac{1}{K} \\sum_{i=1}^{K} \\max_{j \\neq i} R_{ij}$$\n","\n","donde $R_{ij} = \\frac{s_i + s_j}{d(c_i, c_j)}$"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LiWzHYHXW3fu"},"outputs":[],"source":["# Calcular Davies-Bouldin para cada dataset\n","print(\"Índice de Davies-Bouldin\")\n","print(\"=\" * 50)\n","print(\"Rango: [0, +inf), Menor = Mejor\")\n","print(\"=\" * 50)\n","\n","for X, y, titulo in datasets:\n","    kmeans = KMeans(n_clusters=3, random_state=RANDOM_STATE, n_init=10)\n","    labels = kmeans.fit_predict(X)\n","    db_score = davies_bouldin_score(X, labels)\n","    print(f\"\\n{titulo}: {db_score:.4f}\")"]},{"cell_type":"markdown","metadata":{"id":"rZDjj_njW3fv"},"source":["### 2.4 Comparación de Métricas Internas"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HPz2LC-XW3fv"},"outputs":[],"source":["# Comparación completa de métricas internas\n","resultados_internos = []\n","\n","for X, y, titulo in datasets:\n","    kmeans = KMeans(n_clusters=3, random_state=RANDOM_STATE, n_init=10)\n","    labels = kmeans.fit_predict(X)\n","\n","    resultados_internos.append({\n","        'Dataset': titulo,\n","        'Silueta': silhouette_score(X, labels),\n","        'Calinski-Harabasz': calinski_harabasz_score(X, labels),\n","        'Davies-Bouldin': davies_bouldin_score(X, labels)\n","    })\n","\n","df_internos = pd.DataFrame(resultados_internos)\n","print(\"\\nComparación de Métricas Internas:\")\n","print(\"=\" * 70)\n","print(df_internos.to_string(index=False))\n","print(\"\\nNota: Silueta y CH son mejores cuando son mayores; DB es mejor cuando es menor.\")"]},{"cell_type":"markdown","metadata":{"id":"_il-ueobW3fv"},"source":["## 3. Métricas Externas\n","\n","Las métricas externas comparan las etiquetas predichas con etiquetas verdaderas (ground truth)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HtvCm2g0W3fw"},"outputs":[],"source":["# Aplicar diferentes algoritmos de clustering\n","algoritmos = {\n","    'K-Means': KMeans(n_clusters=3, random_state=RANDOM_STATE, n_init=10),\n","    'Hierarchical (Ward)': AgglomerativeClustering(n_clusters=3, linkage='ward'),\n","    'GMM': GaussianMixture(n_components=3, random_state=RANDOM_STATE)\n","}\n","\n","print(\"Métricas Externas - Dataset con clusters bien separados\")\n","print(\"=\" * 80)"]},{"cell_type":"markdown","metadata":{"id":"sXE4BUxEW3fw"},"source":["### 3.1 Adjusted Rand Index (ARI)"]},{"cell_type":"markdown","source":["Probamos resultados de ARI con dataset bien separado"],"metadata":{"id":"FZzWNt7GcFnG"}},{"cell_type":"code","source":["# Usar el dataset de clusters bien separados con ground truth conocido\n","X = X_separados.copy()\n","y_true = y_separados.copy()\n","\n","print(\"Métricas Externas - Dataset con clusters bien separados\")\n","print(\"=\" * 80)\n","\n","print(\"\\nAdjusted Rand Index (ARI)\")\n","print(\"-\" * 50)\n","print(\"Rango: [-1, 1], 1 = perfecto, 0 = aleatorio\")\n","print(\"-\" * 50)\n","\n","for nombre, modelo in algoritmos.items():\n","  labels = modelo.fit_predict(X)\n","  ari = adjusted_rand_score(y_true, labels)\n","  print(f\"{nombre}: {ari:.4f}\")"],"metadata":{"id":"o6gGUHvUb13J"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qk1sI76DW3fx"},"outputs":[],"source":["# Demostración del ajuste por azar en ARI\n","print(\"\\nDemostración: ARI con asignaciones aleatorias\")\n","print(\"-\" * 50)\n","\n","aris_aleatorios = []\n","for i in range(100):\n","    labels_aleatorios = np.random.randint(0, 3, size=len(y_true))\n","    ari = adjusted_rand_score(y_true, labels_aleatorios)\n","    aris_aleatorios.append(ari)\n","\n","print(f\"ARI medio con etiquetas aleatorias (100 iteraciones): {np.mean(aris_aleatorios):.4f}\")\n","\n","print(\"\\nEl ARI está ajustado para que asignaciones aleatorias tengan valor esperado 0.\")"]},{"cell_type":"markdown","source":["Comprobamos que en dataset más solapado, la métrica sale peor"],"metadata":{"id":"eTfElgf6cL_Z"}},{"cell_type":"code","source":["# Usar el dataset de clusters solapados con ground truth conocido\n","X = X_solapados.copy()\n","y_true = y_solapados.copy()\n","\n","print(\"Métricas Externas - Dataset con clusters solapados\")\n","print(\"=\" * 80)\n","\n","print(\"\\nAdjusted Rand Index (ARI)\")\n","\n","for nombre, modelo in algoritmos.items():\n","  labels = modelo.fit_predict(X)\n","  ari = adjusted_rand_score(y_true, labels)\n","  print(f\"{nombre}: {ari:.4f}\")"],"metadata":{"id":"A2IHSBjpbEx3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VEjpuwHrW3fx"},"source":["### 3.2 Normalized Mutual Information (NMI)"]},{"cell_type":"code","source":["# Usar el dataset de clusters bien separados con ground truth conocido\n","X = X_separados.copy()\n","y_true = y_separados.copy()"],"metadata":{"id":"ozrzJHs9chk8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# # O Usar el dataset de clusters solapados con ground truth conocido\n","# X = X_solapados.copy()\n","# y_true = y_solapados.copy()"],"metadata":{"id":"GZfxclHscpRC"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XIGYNdtCW3fy"},"outputs":[],"source":["print(\"\\nNormalized Mutual Information (NMI)\")\n","print(\"-\" * 50)\n","print(\"Rango: [0, 1], 1 = correspondencia perfecta\")\n","print(\"-\" * 50)\n","\n","for nombre, modelo in algoritmos.items():\n","    modelo.fit_predict(X)\n","\n","    nmi = normalized_mutual_info_score(y_true, labels)\n","    print(f\"{nombre}: {nmi:.4f}\")"]},{"cell_type":"markdown","metadata":{"id":"kH7heD5wW3fy"},"source":["### 3.3 Homogeneidad, Completitud y V-measure"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bjY87CFnW3fy"},"outputs":[],"source":["print(\"\\nHomogeneidad, Completitud y V-measure\")\n","print(\"-\" * 70)\n","print(\"Homogeneidad: cada cluster contiene solo miembros de una clase\")\n","print(\"Completitud: todos los miembros de una clase están en el mismo cluster\")\n","print(\"V-measure: media armónica de homogeneidad y completitud\")\n","print(\"-\" * 70)\n","\n","for nombre, modelo in algoritmos.items():\n","  labels = modelo.fit_predict(X)\n","  h = homogeneity_score(y_true, labels)\n","  c = completeness_score(y_true, labels)\n","  v = v_measure_score(y_true, labels)\n","\n","  print(f\"\\n{nombre}:\")\n","  print(f\"  Homogeneidad: {h:.4f}\")\n","  print(f\"  Completitud:  {c:.4f}\")\n","  print(f\"  V-measure:    {v:.4f}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qGjHgI1dW3fz"},"outputs":[],"source":["# Demostración de la diferencia entre homogeneidad y completitud\n","print(\"\\nDemostración: Casos extremos de homogeneidad y completitud\")\n","print(\"=\" * 60)\n","\n","# Caso 1: Muchos clusters pequeños (alta homogeneidad, baja completitud)\n","labels_muchos = np.arange(len(y_true))  # Cada punto es su propio cluster\n","print(\"\\nCaso: Cada punto es su propio cluster\")\n","print(f\"  Homogeneidad: {homogeneity_score(y_true, labels_muchos):.4f} (perfecta)\")\n","print(f\"  Completitud:  {completeness_score(y_true, labels_muchos):.4f} (mínima)\")\n","\n","# Caso 2: Un solo cluster (baja homogeneidad, alta completitud)\n","labels_uno = np.zeros(len(y_true), dtype=int)\n","print(\"\\nCaso: Todos en un solo cluster\")\n","print(f\"  Homogeneidad: {homogeneity_score(y_true, labels_uno):.4f} (mínima)\")\n","print(f\"  Completitud:  {completeness_score(y_true, labels_uno):.4f} (perfecta)\")"]},{"cell_type":"markdown","metadata":{"id":"dGYxwv2GW3fz"},"source":["### 3.4 Fowlkes-Mallows Index (FMI)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CaYeJkyyW3fz"},"outputs":[],"source":["print(\"\\nFowlkes-Mallows Index (FMI)\")\n","print(\"-\" * 50)\n","print(\"Rango: [0, 1], 1 = clustering perfecto\")\n","print(\"Media geométrica de precision y recall sobre pares\")\n","print(\"-\" * 50)\n","\n","for nombre, modelo in algoritmos.items():\n","  labels = modelo.fit_predict(X)\n","  fmi = fowlkes_mallows_score(y_true, labels)\n","  print(f\"{nombre}: {fmi:.4f}\")"]},{"cell_type":"markdown","metadata":{"id":"G0BmZp1mW3fz"},"source":["### 3.5 Comparación Completa de Métricas Externas"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cRWHZK7gW3f0"},"outputs":[],"source":["# Tabla comparativa completa\n","resultados_externos = []\n","\n","for nombre, modelo in algoritmos.items():\n","    labels = modelo.fit_predict(X)\n","\n","    resultados_externos.append({\n","        'Algoritmo': nombre,\n","        'ARI': adjusted_rand_score(y_true, labels),\n","        'NMI': normalized_mutual_info_score(y_true, labels),\n","        'Homog.': homogeneity_score(y_true, labels),\n","        'Complet.': completeness_score(y_true, labels),\n","        'V-measure': v_measure_score(y_true, labels),\n","        'FMI': fowlkes_mallows_score(y_true, labels)\n","    })\n","\n","df_externos = pd.DataFrame(resultados_externos)\n","print(\"\\nComparación completa de métricas externas:\")\n","print(\"=\" * 80)\n","print(df_externos.to_string(index=False))"]},{"cell_type":"code","source":["# O Usar el dataset de clusters solapados con ground truth conocido\n","X = X_solapados.copy()\n","y_true = y_solapados.copy()\n","\n","# Tabla comparativa completa\n","resultados_externos = []\n","\n","for nombre, modelo in algoritmos.items():\n","    labels = modelo.fit_predict(X)\n","\n","    resultados_externos.append({\n","        'Algoritmo': nombre,\n","        'ARI': adjusted_rand_score(y_true, labels),\n","        'NMI': normalized_mutual_info_score(y_true, labels),\n","        'Homog.': homogeneity_score(y_true, labels),\n","        'Complet.': completeness_score(y_true, labels),\n","        'V-measure': v_measure_score(y_true, labels),\n","        'FMI': fowlkes_mallows_score(y_true, labels)\n","    })\n","\n","df_externos = pd.DataFrame(resultados_externos)\n","print(\"\\nComparación completa de métricas externas:\")\n","print(\"=\" * 80)\n","print(df_externos.to_string(index=False))"],"metadata":{"id":"LXekWW8lex_i"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZjKwgn_KW3f0"},"source":["## 4. Selección del Número de Clusters\n","\n","Utilizamos métricas internas para determinar el número óptimo de clusters K."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xCQE6mvkW3f0"},"outputs":[],"source":["# Generar dataset para selección de K\n","np.random.seed(RANDOM_STATE)\n","X_seleccion, y_seleccion = make_blobs(n_samples=500, centers=5,\n","                                       cluster_std=1.0, random_state=RANDOM_STATE)\n","\n","# Calcular métricas para diferentes valores de K\n","rango_k = range(2, 11)\n","metricas_k = {\n","    'Silueta': [],\n","    'Calinski-Harabasz': [],\n","    'Davies-Bouldin': [],\n","    'Inercia': []\n","}\n","\n","for k in rango_k:\n","    kmeans = KMeans(n_clusters=k, random_state=RANDOM_STATE, n_init=10)\n","    labels = kmeans.fit_predict(X_seleccion)\n","\n","    metricas_k['Silueta'].append(silhouette_score(X_seleccion, labels))\n","    metricas_k['Calinski-Harabasz'].append(calinski_harabasz_score(X_seleccion, labels))\n","    metricas_k['Davies-Bouldin'].append(davies_bouldin_score(X_seleccion, labels))\n","    metricas_k['Inercia'].append(kmeans.inertia_)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wro0raBdW3f1"},"outputs":[],"source":["# Visualización de métricas vs K\n","fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n","\n","# Silueta\n","ax = axes[0, 0]\n","ax.plot(list(rango_k), metricas_k['Silueta'], 'b-o', linewidth=2, markersize=8)\n","ax.axvline(x=5, color='red', linestyle='--', alpha=0.7, label='K real = 5')\n","k_opt_sil = list(rango_k)[np.argmax(metricas_k['Silueta'])]\n","ax.axvline(x=k_opt_sil, color='green', linestyle=':', alpha=0.7, label=f'K óptimo = {k_opt_sil}')\n","ax.set_xlabel('Número de clusters (K)')\n","ax.set_ylabel('Silueta')\n","ax.set_title('Coeficiente de Silueta (Mayor = Mejor)')\n","ax.legend()\n","ax.grid(True, alpha=0.3)\n","\n","# Calinski-Harabasz\n","ax = axes[0, 1]\n","ax.plot(list(rango_k), metricas_k['Calinski-Harabasz'], 'g-o', linewidth=2, markersize=8)\n","ax.axvline(x=5, color='red', linestyle='--', alpha=0.7, label='K real = 5')\n","k_opt_ch = list(rango_k)[np.argmax(metricas_k['Calinski-Harabasz'])]\n","ax.axvline(x=k_opt_ch, color='green', linestyle=':', alpha=0.7, label=f'K óptimo = {k_opt_ch}')\n","ax.set_xlabel('Número de clusters (K)')\n","ax.set_ylabel('Calinski-Harabasz')\n","ax.set_title('Índice de Calinski-Harabasz (Mayor = Mejor)')\n","ax.legend()\n","ax.grid(True, alpha=0.3)\n","\n","# Davies-Bouldin\n","ax = axes[1, 0]\n","ax.plot(list(rango_k), metricas_k['Davies-Bouldin'], 'r-o', linewidth=2, markersize=8)\n","ax.axvline(x=5, color='red', linestyle='--', alpha=0.7, label='K real = 5')\n","k_opt_db = list(rango_k)[np.argmin(metricas_k['Davies-Bouldin'])]\n","ax.axvline(x=k_opt_db, color='green', linestyle=':', alpha=0.7, label=f'K óptimo = {k_opt_db}')\n","ax.set_xlabel('Número de clusters (K)')\n","ax.set_ylabel('Davies-Bouldin')\n","ax.set_title('Índice de Davies-Bouldin (Menor = Mejor)')\n","ax.legend()\n","ax.grid(True, alpha=0.3)\n","\n","# Método del codo (Inercia)\n","ax = axes[1, 1]\n","ax.plot(list(rango_k), metricas_k['Inercia'], 'm-o', linewidth=2, markersize=8)\n","ax.axvline(x=5, color='red', linestyle='--', alpha=0.7, label='K real = 5')\n","ax.set_xlabel('Número de clusters (K)')\n","ax.set_ylabel('Inercia (WCSS)')\n","ax.set_title('Método del Codo')\n","ax.legend()\n","ax.grid(True, alpha=0.3)\n","\n","plt.suptitle('Selección del número óptimo de clusters', fontsize=14, fontweight='bold')\n","plt.tight_layout()\n","plt.show()\n","\n","print(f\"\\nK óptimo según cada métrica:\")\n","print(f\"  Silueta: {k_opt_sil}\")\n","print(f\"  Calinski-Harabasz: {k_opt_ch}\")\n","print(f\"  Davies-Bouldin: {k_opt_db}\")\n","print(f\"  K real: 5\")"]},{"cell_type":"markdown","metadata":{"id":"pJ-KJvvoW3f1"},"source":["## 5. Estabilidad de Clusters\n","\n","Evaluamos la robustez del clustering mediante técnicas de remuestreo (bootstrap)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"C-A9Dq_NW3f2"},"outputs":[],"source":["def evaluar_estabilidad_bootstrap(X, n_clusters, n_bootstrap=50, subsample_ratio=0.8):\n","    \"\"\"\n","    Evalúa la estabilidad del clustering mediante bootstrap.\n","\n","    Retorna:\n","    - Lista de ARI entre clusterings de diferentes muestras bootstrap\n","    \"\"\"\n","    n_samples = len(X)\n","    subsample_size = int(n_samples * subsample_ratio)\n","\n","    # Almacenar resultados de cada iteración\n","    all_labels = []\n","    all_indices = []\n","\n","    for i in range(n_bootstrap):\n","        # Submuestra aleatoria\n","        indices = np.random.choice(n_samples, size=subsample_size, replace=False)\n","        X_sub = X[indices]\n","\n","        # Clustering\n","        kmeans = KMeans(n_clusters=n_clusters, random_state=i, n_init=10)\n","        labels = kmeans.fit_predict(X_sub)\n","\n","        all_labels.append(labels)\n","        all_indices.append(indices)\n","\n","    # Calcular ARI entre pares de particiones (en puntos comunes)\n","    aris = []\n","    for i in range(n_bootstrap):\n","        for j in range(i+1, n_bootstrap):\n","            # Encontrar puntos en común\n","            common = np.intersect1d(all_indices[i], all_indices[j])\n","            if len(common) > 10:\n","                # Obtener etiquetas para puntos comunes\n","                mask_i = np.isin(all_indices[i], common)\n","                mask_j = np.isin(all_indices[j], common)\n","\n","                labels_i = all_labels[i][mask_i]\n","                labels_j = all_labels[j][mask_j]\n","\n","                # Reordenar para que coincidan los índices\n","                order_i = np.argsort(all_indices[i][mask_i])\n","                order_j = np.argsort(all_indices[j][mask_j])\n","\n","                ari = adjusted_rand_score(labels_i[order_i], labels_j[order_j])\n","                aris.append(ari)\n","\n","    return aris"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NXuzIBQ4W3f3"},"outputs":[],"source":["# Evaluar estabilidad para diferentes valores de K\n","print(\"Análisis de estabilidad mediante bootstrap\")\n","print(\"=\" * 60)\n","\n","# Usar el dataset de 5 clusters\n","estabilidades = {}\n","\n","for k in [3, 4, 5, 6, 7]:\n","    aris = evaluar_estabilidad_bootstrap(X_seleccion, n_clusters=k, n_bootstrap=30)\n","    estabilidades[k] = aris\n","    print(f\"K={k}: Estabilidad media = {np.mean(aris):.4f} (+/- {np.std(aris):.4f})\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DQdfk9MBW3f3"},"outputs":[],"source":["# Visualización de estabilidad\n","fig, ax = plt.subplots(figsize=(12, 6))\n","\n","ks = list(estabilidades.keys())\n","means = [np.mean(estabilidades[k]) for k in ks]\n","stds = [np.std(estabilidades[k]) for k in ks]\n","\n","ax.errorbar(ks, means, yerr=stds, fmt='o-', linewidth=2, markersize=10,\n","            capsize=5, capthick=2)\n","ax.axvline(x=5, color='red', linestyle='--', alpha=0.7, label='K real = 5')\n","ax.axhline(y=1.0, color='gray', linestyle=':', alpha=0.5)\n","\n","ax.set_xlabel('Número de clusters (K)')\n","ax.set_ylabel('Estabilidad (ARI medio entre bootstrap)')\n","ax.set_title('Estabilidad del clustering vs número de clusters')\n","ax.legend()\n","ax.grid(True, alpha=0.3)\n","ax.set_ylim([0, 1.1])\n","\n","plt.tight_layout()\n","plt.show()\n","\n","print(\"\\nInterpretación:\")\n","print(\"- Alta estabilidad (~1.0) indica que el clustering es robusto\")\n","print(\"- El K óptimo suele tener la mayor estabilidad\")\n","print(\"- Varianza alta indica sensibilidad a los datos de entrada\")"]},{"cell_type":"markdown","metadata":{"id":"iSfdtmIBW3f4"},"source":["## 6. Caso Práctico: Dataset Wine\n","\n","Aplicamos todas las métricas a un dataset real multidimensional."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WBFmHg7RW3f4"},"outputs":[],"source":["# Cargar y preparar dataset Wine\n","wine = load_wine()\n","X_wine = wine.data\n","y_wine = wine.target\n","\n","# Estandarizar\n","scaler = StandardScaler()\n","X_wine_scaled = scaler.fit_transform(X_wine)\n","\n","# Reducir a 2D para visualización\n","pca = PCA(n_components=2)\n","X_wine_2d = pca.fit_transform(X_wine_scaled)\n","\n","print(f\"Dataset Wine:\")\n","print(f\"  Muestras: {X_wine.shape[0]}\")\n","print(f\"  Características: {X_wine.shape[1]}\")\n","print(f\"  Clases reales: {len(np.unique(y_wine))}\")\n","print(f\"  Varianza explicada por 2 PCs: {pca.explained_variance_ratio_.sum():.2%}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DxJmePPsW3f5"},"outputs":[],"source":["# Comparación completa de algoritmos\n","algoritmos_wine = {\n","    'K-Means': KMeans(n_clusters=3, random_state=RANDOM_STATE, n_init=10),\n","    'Hierarchical': AgglomerativeClustering(n_clusters=3, linkage='ward'),\n","    'GMM': GaussianMixture(n_components=3, random_state=RANDOM_STATE),\n","}\n","\n","resultados_wine = []\n","\n","for nombre, modelo in algoritmos_wine.items():\n","    labels = modelo.fit_predict(X_wine_scaled)\n","\n","    resultado = {\n","        'Algoritmo': nombre,\n","        # Métricas internas\n","        'Silueta': silhouette_score(X_wine_scaled, labels),\n","        'CH': calinski_harabasz_score(X_wine_scaled, labels),\n","        'DB': davies_bouldin_score(X_wine_scaled, labels),\n","        # Métricas externas\n","        'ARI': adjusted_rand_score(y_wine, labels),\n","        'NMI': normalized_mutual_info_score(y_wine, labels),\n","        'V-measure': v_measure_score(y_wine, labels),\n","        'FMI': fowlkes_mallows_score(y_wine, labels)\n","    }\n","    resultados_wine.append(resultado)\n","\n","df_wine = pd.DataFrame(resultados_wine)\n","print(\"\\nResultados en dataset Wine:\")\n","print(\"=\" * 100)\n","print(df_wine.to_string(index=False))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QPcKa00qW3gA"},"outputs":[],"source":["# Visualización comparativa\n","fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n","\n","# Ground truth\n","ax = axes[0, 0]\n","ax.scatter(X_wine_2d[:, 0], X_wine_2d[:, 1], c=y_wine, cmap='viridis',\n","           edgecolors='w', s=50)\n","ax.set_title('Ground Truth')\n","ax.set_xlabel('PC1')\n","ax.set_ylabel('PC2')\n","\n","# K-Means\n","ax = axes[0, 1]\n","labels_km = KMeans(n_clusters=3, random_state=RANDOM_STATE, n_init=10).fit_predict(X_wine_scaled)\n","ax.scatter(X_wine_2d[:, 0], X_wine_2d[:, 1], c=labels_km, cmap='viridis',\n","           edgecolors='w', s=50)\n","ari_km = adjusted_rand_score(y_wine, labels_km)\n","sil_km = silhouette_score(X_wine_scaled, labels_km)\n","ax.set_title(f'K-Means\\nARI: {ari_km:.3f}, Silueta: {sil_km:.3f}')\n","ax.set_xlabel('PC1')\n","ax.set_ylabel('PC2')\n","\n","# Hierarchical\n","ax = axes[1, 0]\n","labels_hc = AgglomerativeClustering(n_clusters=3, linkage='ward').fit_predict(X_wine_scaled)\n","ax.scatter(X_wine_2d[:, 0], X_wine_2d[:, 1], c=labels_hc, cmap='viridis',\n","           edgecolors='w', s=50)\n","ari_hc = adjusted_rand_score(y_wine, labels_hc)\n","sil_hc = silhouette_score(X_wine_scaled, labels_hc)\n","ax.set_title(f'Hierarchical (Ward)\\nARI: {ari_hc:.3f}, Silueta: {sil_hc:.3f}')\n","ax.set_xlabel('PC1')\n","ax.set_ylabel('PC2')\n","\n","# GMM\n","ax = axes[1, 1]\n","labels_gmm = GaussianMixture(n_components=3, random_state=RANDOM_STATE).fit_predict(X_wine_scaled)\n","ax.scatter(X_wine_2d[:, 0], X_wine_2d[:, 1], c=labels_gmm, cmap='viridis',\n","           edgecolors='w', s=50)\n","ari_gmm = adjusted_rand_score(y_wine, labels_gmm)\n","sil_gmm = silhouette_score(X_wine_scaled, labels_gmm)\n","ax.set_title(f'GMM\\nARI: {ari_gmm:.3f}, Silueta: {sil_gmm:.3f}')\n","ax.set_xlabel('PC1')\n","ax.set_ylabel('PC2')\n","\n","plt.suptitle('Comparación de algoritmos en dataset Wine', fontsize=14, fontweight='bold')\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"j2o2rI2oW3gB"},"outputs":[],"source":["# Análisis de silueta detallado para K-Means en Wine\n","plot_silhouette_analysis(X_wine_scaled, labels_km, 'Análisis de silueta - K-Means en Wine')"]},{"cell_type":"markdown","metadata":{"id":"ubZQ7wpQW3gC"},"source":["## 7. Métricas en Escenarios Problemáticos\n","\n","Evaluamos cómo se comportan las métricas cuando el clustering falla."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0nB_PUsUW3gC"},"outputs":[],"source":["# Dataset con estructura no convexa (moons)\n","X_moons, y_moons = make_moons(n_samples=300, noise=0.1, random_state=RANDOM_STATE)\n","\n","# Aplicar K-Means (que fallará) vs el clustering real\n","kmeans_moons = KMeans(n_clusters=2, random_state=RANDOM_STATE, n_init=10)\n","labels_kmeans_moons = kmeans_moons.fit_predict(X_moons)\n","\n","fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n","\n","# Ground truth\n","ax = axes[0]\n","ax.scatter(X_moons[:, 0], X_moons[:, 1], c=y_moons, cmap='viridis',\n","           edgecolors='w', s=50)\n","ax.set_title('Ground Truth')\n","ax.set_xlabel('X1')\n","ax.set_ylabel('X2')\n","\n","# K-Means\n","ax = axes[1]\n","ax.scatter(X_moons[:, 0], X_moons[:, 1], c=labels_kmeans_moons, cmap='viridis',\n","           edgecolors='w', s=50)\n","ax.scatter(kmeans_moons.cluster_centers_[:, 0], kmeans_moons.cluster_centers_[:, 1],\n","           c='red', marker='X', s=200, edgecolors='w')\n","ax.set_title('K-Means (clustering incorrecto)')\n","ax.set_xlabel('X1')\n","ax.set_ylabel('X2')\n","\n","plt.suptitle('K-Means en datos no convexos (moons)', fontsize=14, fontweight='bold')\n","plt.tight_layout()\n","plt.show()\n","\n","# Calcular métricas\n","print(\"\\nMétricas para K-Means en dataset moons:\")\n","print(\"=\" * 50)\n","print(\"\\nMétricas internas (sin ground truth):\")\n","print(f\"  Silueta: {silhouette_score(X_moons, labels_kmeans_moons):.4f}\")\n","print(f\"  Calinski-Harabasz: {calinski_harabasz_score(X_moons, labels_kmeans_moons):.2f}\")\n","print(f\"  Davies-Bouldin: {davies_bouldin_score(X_moons, labels_kmeans_moons):.4f}\")\n","\n","print(\"\\nMétricas externas (con ground truth):\")\n","print(f\"  ARI: {adjusted_rand_score(y_moons, labels_kmeans_moons):.4f}\")\n","print(f\"  NMI: {normalized_mutual_info_score(y_moons, labels_kmeans_moons):.4f}\")\n","\n","print(\"\\nNota: Las métricas internas pueden dar valores 'buenos' incluso cuando\")\n","print(\"el clustering es incorrecto, porque asumen clusters convexos.\")"]},{"cell_type":"markdown","metadata":{"id":"0gg1azDWW3gC"},"source":["## 8. Resumen y Conclusiones\n","\n","### Guía de Selección de Métricas\n","\n","**Métricas Internas (sin ground truth):**\n","\n","| Métrica | Rango | Óptimo | Uso Principal | Limitación |\n","|---------|-------|--------|---------------|------------|\n","| Silueta | [-1, 1] | Mayor | Diagnóstico por punto | O(n²) |\n","| Calinski-Harabasz | [0, +∞) | Mayor | Selección de K rápida | Asume convexidad |\n","| Davies-Bouldin | [0, +∞) | Menor | Detectar clusters similares | Solo usa centroides |\n","\n","**Métricas Externas (con ground truth):**\n","\n","| Métrica | Rango | Ajustada | Uso Principal |\n","|---------|-------|----------|---------------|\n","| ARI | [-1, 1] | Sí | Benchmarking general |\n","| NMI | [0, 1] | No (AMI sí) | Comparar particiones |\n","| V-measure | [0, 1] | No | Diagnóstico h/c |\n","| FMI | [0, 1] | No | Balance precision/recall |\n","\n","### Recomendaciones Prácticas\n","\n","1. **Usar múltiples métricas**: ninguna métrica es perfecta\n","2. **Preferir métricas ajustadas** (ARI, AMI) para comparaciones justas\n","3. **Complementar con visualización** siempre que sea posible\n","4. **Evaluar estabilidad** mediante bootstrap para clusterings robustos\n","5. **Considerar el contexto del problema**: la interpretabilidad a veces importa más que las métricas"]},{"cell_type":"markdown","metadata":{"id":"YflJQ_0ZW3gD"},"source":["---\n","\n","## Referencias\n","\n","- Rousseeuw, P. J. (1987). Silhouettes: a graphical aid to the interpretation and validation of cluster analysis. Journal of Computational and Applied Mathematics, 20, 53-65.\n","- Calinski, T., & Harabasz, J. (1974). A dendrite method for cluster analysis. Communications in Statistics, 3(1), 1-27.\n","- Davies, D. L., & Bouldin, D. W. (1979). A cluster separation measure. IEEE TPAMI, 1(2), 224-227.\n","- Hubert, L., & Arabie, P. (1985). Comparing partitions. Journal of Classification, 2(1), 193-218.\n","- Vinh, N. X., Epps, J., & Bailey, J. (2010). Information theoretic measures for clusterings comparison. ICML.\n","- Scikit-learn documentation: https://scikit-learn.org/stable/modules/clustering.html#clustering-evaluation\n","\n","---\n"]},{"cell_type":"markdown","source":["# EOF (End Of File)"],"metadata":{"id":"mHJT2j2Wf11c"}}]}